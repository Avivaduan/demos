{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, Random, LinearAlgebra, Statistics, SparseArrays\n",
    "include(\"proxgrad.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving ERM problems\n",
    "\n",
    "The file `proxgrad.jl` contains code for solving regularized empirical risk minimization (ERM) problems. It provides the optimization function `proxgrad` together with a large number of predefined loss functions and regularizers.\n",
    "    \n",
    "The function `proxgrad` solves regularized ERM problems of the form\n",
    "$$\n",
    "\\mbox{minimize} \\quad \\sum_{i=1}^n \\ell(y_i, w^T x_i) + r(w).    \n",
    "$$\n",
    "It solves these with the proximal gradient method, which we will learn shortly.\n",
    "\n",
    "You can select from a range of losses. For real valued $y$, try:\n",
    "   * quadratic loss - `QuadLoss()`\n",
    "   * $\\ell_1$ loss - `L1Loss()`\n",
    "   * quantile loss (for $\\alpha$ quantile) - `QuantileLoss(α)`\n",
    " \n",
    "For Boolean $y$, try\n",
    "   * hinge loss - `HingeLoss()`\n",
    "   * logistic loss - `LogisticLoss()`\n",
    "   * weighted hinge loss - `WeightedHingeLoss()`\n",
    "\n",
    "For nominal $y$, try\n",
    "   * multinomial loss - `MultinomialLoss()`\n",
    "   * one vs all loss - `OvALoss()`\n",
    "       * (by default, it uses the logistic loss for the underlying binary classifier)\n",
    "\n",
    "For ordinal $y$, try\n",
    "   * ordinal hinge loss - `OrdinalHingeLoss()`\n",
    "   * bigger vs smaller loss - `BvSLoss()`\n",
    "       * (by default, it uses the logistic loss for the underlying binary classifier)\n",
    "       \n",
    "It also provides a few regularizers, including \n",
    "   * no regularization - `ZeroReg()`\n",
    "   * quadratic regularization - `QuadReg()`\n",
    "   * $\\ell_1$ regularization - `OneReg()`\n",
    "   * nonnegative constraint - `NonNegConstraint()`\n",
    "       \n",
    "Below, we provide some examples for how to use the proxgrad function to fit regularized ERM problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate random data set\n",
    "\n",
    "First (as usual), we'll generate some random data to try our methods on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(0)\n",
    "n = 50\n",
    "d = 10\n",
    "X = randn(n,d)\n",
    "w♮ = randn(d)\n",
    "y = X*w♮ + .1*randn(n);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic loss, quadratic regularizer\n",
    "\n",
    "$$\n",
    "\\mbox{minimize} \\quad \\frac 1 n ||Xw - y||^2 + λ||w||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06951902408490734"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we form \\frac 1 n || ⋅ ||^2 by multiplying the QuadLoss() function by 1/n\n",
    "loss = 1/n*QuadLoss()\n",
    "\n",
    "# we form λ|| ⋅ ||^2 by multiplying the QuadReg() function by λ\n",
    "λ = .1\n",
    "reg = λ*QuadReg()\n",
    "\n",
    "# minimize 1/n ||Xw - y||^2 + λ||w||^2\n",
    "w = proxgrad(loss, reg, X, y, maxiters=7) \n",
    "\n",
    "norm(X*w-y) / norm(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`maxiters`, the maximum number of iterations, controls how fully we converge.\n",
    "You can try increasing it to see if the error improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching evaluate(::WeightedHingeLoss, ::Float64, ::Float64)\nClosest candidates are:\n  evaluate(::WeightedHingeLoss, ::Float64, !Matched::Bool) at /Users/madeleine/.julia/packages/LowRankModels/uhtB9/src/losses.jl:327\n  evaluate(!Matched::QuadLoss, ::Float64, ::Number) at /Users/madeleine/.julia/packages/LowRankModels/uhtB9/src/losses.jl:144\n  evaluate(!Matched::L1Loss, ::Float64, ::Number) at /Users/madeleine/.julia/packages/LowRankModels/uhtB9/src/losses.jl:158\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching evaluate(::WeightedHingeLoss, ::Float64, ::Float64)\nClosest candidates are:\n  evaluate(::WeightedHingeLoss, ::Float64, !Matched::Bool) at /Users/madeleine/.julia/packages/LowRankModels/uhtB9/src/losses.jl:327\n  evaluate(!Matched::QuadLoss, ::Float64, ::Number) at /Users/madeleine/.julia/packages/LowRankModels/uhtB9/src/losses.jl:144\n  evaluate(!Matched::L1Loss, ::Float64, ::Number) at /Users/madeleine/.julia/packages/LowRankModels/uhtB9/src/losses.jl:158\n  ...",
      "",
      "Stacktrace:",
      " [1] (::getfield(LowRankModels, Symbol(\"##31#32\")){WeightedHingeLoss})(::Float64, ::Float64) at /Users/madeleine/.julia/packages/LowRankModels/uhtB9/src/losses.jl:653",
      " [2] map! at ./abstractarray.jl:2108 [inlined]",
      " [3] evaluate(::WeightedHingeLoss, ::Array{Float64,1}, ::Array{Float64,1}) at /Users/madeleine/.julia/packages/LowRankModels/uhtB9/src/losses.jl:655",
      " [4] evaluate at /Users/madeleine/Documents/4741/demos/proxgrad-ls.jl:6 [inlined]",
      " [5] #proxgrad_linesearch#20(::Int64, ::Float64, ::Float64, ::Array{Float64,1}, ::ConvergenceHistory, ::typeof(proxgrad_linesearch), ::WeightedHingeLoss, ::QuadReg, ::Array{Float64,2}, ::Array{Float64,1}) at /Users/madeleine/Documents/4741/demos/proxgrad-ls.jl:43",
      " [6] (::getfield(Main, Symbol(\"#kw##proxgrad_linesearch\")))(::NamedTuple{(:maxiters,),Tuple{Int64}}, ::typeof(proxgrad_linesearch), ::WeightedHingeLoss, ::QuadReg, ::Array{Float64,2}, ::Array{Float64,1}) at ./none:0",
      " [7] #proxgrad#19(::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxiters,),Tuple{Int64}}}, ::typeof(proxgrad), ::WeightedHingeLoss, ::QuadReg, ::Vararg{Any,N} where N) at /Users/madeleine/Documents/4741/demos/proxgrad-ls.jl:29",
      " [8] (::getfield(Main, Symbol(\"#kw##proxgrad\")))(::NamedTuple{(:maxiters,),Tuple{Int64}}, ::typeof(proxgrad), ::WeightedHingeLoss, ::QuadReg, ::Vararg{Any,N} where N) at ./none:0",
      " [9] top-level scope at In[53]:1"
     ]
    }
   ],
   "source": [
    "w = proxgrad(loss, reg, X, y, maxiters=10) \n",
    "norm(X*w-y) / norm(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge loss, quadratic regularizer\n",
    "\n",
    "$$\n",
    "\\mbox{minimize} \\quad \\frac 1 n \\sum_{i=1}^n (1 - y_i w^T x_i)_+ + λ||w||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ybool = Int.(sign.(y)) # form a boolean target\n",
    "\n",
    "# we form \\frac 1 n \\sum_{i=1}^n (1 - ⋅ )_+ by multiplying the HingeLoss() function by 1/n\n",
    "loss = 1/n*HingeLoss()\n",
    "\n",
    "# we form λ|| ⋅ ||^2 by multiplying the QuadReg() function by λ\n",
    "λ = .1\n",
    "reg = λ*QuadReg()\n",
    "\n",
    "# minimize 1/n \\frac 1 n \\sum_{i=1}^n (1 - y_i w^T x_i)_+ + λ||w||^2\n",
    "w = proxgrad(loss, reg, X, ybool, maxiters=10) \n",
    "\n",
    "# misclassification error \n",
    "(n - sum(sign.(X*w) .== ybool)) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework question \n",
    "\n",
    "Use the proxgrad function to fit the following objective\n",
    "    \n",
    "$$\n",
    "\\mbox{minimize} \\quad \\frac 1 n \\sum_{i=1}^n \\log(1 + \\exp(- \\text{ybool}_i w^T x_i)) + λ||w||^2\n",
    "$$\n",
    "for $\\lambda = .5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
