{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "proxgrad_const (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots, Random, LinearAlgebra, Statistics, SparseArrays\n",
    "include(\"proxgrad.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving ERM problems\n",
    "\n",
    "The file `proxgrad.jl` contains code for solving regularized empirical risk minimization (ERM) problems. It provides the optimization function `proxgrad` together with a large number of predefined loss functions and regularizers.\n",
    "    \n",
    "The function `proxgrad` solves regularized ERM problems of the form\n",
    "$$\n",
    "\\mbox{minimize} \\quad \\sum_{i=1}^n \\ell(y_i, w^T x_i) + r(w).    \n",
    "$$\n",
    "It solves these with the proximal gradient method, which we will learn shortly.\n",
    "\n",
    "You can select from a range of losses. For real valued $y$, try:\n",
    "   * quadratic loss - `QuadLoss()`\n",
    "   * $\\ell_1$ loss - `L1Loss()`\n",
    "   * quantile loss (for $\\alpha$ quantile) - `QuantileLoss(α)`\n",
    " \n",
    "For Boolean $y$, try\n",
    "   * hinge loss - `HingeLoss()`\n",
    "   * logistic loss - `LogisticLoss()`\n",
    "   * weighted hinge loss - `WeightedHingeLoss()`\n",
    "\n",
    "For nominal $y$, try\n",
    "   * multinomial loss - `MultinomialLoss()`\n",
    "   * one vs all loss - `OvALoss()`\n",
    "       * (by default, it uses the logistic loss for the underlying binary classifier)\n",
    "\n",
    "For ordinal $y$, try\n",
    "   * ordinal hinge loss - `OrdinalHingeLoss()`\n",
    "   * bigger vs smaller loss - `BvSLoss()`\n",
    "       * (by default, it uses the logistic loss for the underlying binary classifier)\n",
    "       \n",
    "It also provides a few regularizers, including \n",
    "   * quadratic regularization - `QuadReg()`\n",
    "   * $\\ell_1$ regularization - `OneReg()`\n",
    "   * nonnegative constraint - `NonNegConstraint()`\n",
    "       \n",
    "Below, we provide some examples for how to use the proxgrad function to fit regularized ERM problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate random data set\n",
    "\n",
    "First (as usual), we'll generate some random data to try our methods on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(0)\n",
    "n = 50\n",
    "d = 10\n",
    "X = randn(n,d)\n",
    "w♮ = randn(d)\n",
    "y = X*w♮ + randn(n);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic loss, quadratic regularizer\n",
    "\n",
    "$$\n",
    "\\mbox{minimize} \\quad \\frac 1 n ||Xw - y||^2 + λ||w||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.257873738852037"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we form \\frac 1 n || ⋅ ||^2 by multiplying the QuadLoss() function by 1/n\n",
    "loss = 1/n*QuadLoss()\n",
    "\n",
    "# we form λ|| ⋅ ||^2 by multiplying the QuadReg() function by λ\n",
    "λ = .1\n",
    "reg = λ*QuadReg()\n",
    "\n",
    "# minimize 1/n ||Xw - y||^2 + λ||w||^2\n",
    "w = proxgrad(loss, reg, X, y, maxiters=10) \n",
    "\n",
    "norm(X*w-y) / norm(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`maxiters`, the maximum number of iterations, controls how fully we converge.\n",
    "You can try increasing it to see if the error improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2607432004382123"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = proxgrad(loss, reg, X, y, maxiters=100) \n",
    "norm(X*w-y) / norm(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge loss, quadratic regularizer\n",
    "\n",
    "$$\n",
    "\\mbox{minimize} \\quad \\frac 1 n \\sum_{i=1}^n (1 - y_i w^T x_i)_+ + λ||w||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7258075872083293"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ybool = Int.(sign.(y)) # form a boolean target\n",
    "\n",
    "# we form \\frac 1 n \\sum_{i=1}^n (1 - ⋅ )_+ by multiplying the HingeLoss() function by 1/n\n",
    "loss = 1/n*HingeLoss()\n",
    "\n",
    "# we form λ|| ⋅ ||^2 by multiplying the QuadReg() function by λ\n",
    "λ = .1\n",
    "reg = λ*QuadReg()\n",
    "\n",
    "# minimize 1/n \\frac 1 n \\sum_{i=1}^n (1 - y_i w^T x_i)_+ + λ||w||^2\n",
    "w = proxgrad(loss, reg, X, ybool, maxiters=100) \n",
    "\n",
    "norm(X*w-y) / norm(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework question \n",
    "\n",
    "Use the proxgrad function to fit the following objective\n",
    "    \n",
    "$$\n",
    "\\mbox{minimize} \\quad \\frac 1 n \\sum_{i=1}^n \\log(1 + \\exp(- \\text{ybool}_i w^T x_i)) + λ||w||^2\n",
    "$$\n",
    "for $\\lambda = .5$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
